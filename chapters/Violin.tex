% !TeX spellcheck = en_US
\chapter{Automatic description of the timbre of violins}
\label{Chap:Violin}
The formalization of the semantic domain concerns the different high-level aspects of music. We can consider the aspects of whole songs as well as the aspects of specific music components. In this Chapter we investigate the automatic description of the sound quality properties of the violin instruments, i.e., their \textit{timbral} properties. 

The study of the timbral qualities of violins has been the subject of intense scientific investigation \cite{Woodhouse2014} for decades. However, the physical phenomena that are involved in the characterization of their timbral quality are still far from being fully understood \cite{Zanoni2014}. This recently motivated the start of a new research projects in Cremona, place of birth of the violins, with the Politecnico di Milano (for aspects of musical acoustics) and the University of Pavia (for aspects of material analysis), aimed at exploring new directions in contemporary lutherie. Among the many goals of the projects are the investigation of the timbral quality of violins and, in particular, understanding the links that exist between objective and semantic descriptors related to such instruments. The former are geometric, vibro-acoustic, and acoustic features; physical and chemical properties of materials, etc. The latter are the terms of natural language that are customarily used for describing qualities of the instrument. 

The violin is one of the most ancient and complex instruments in the classical Western music tradition. The semantics regarding its sound quality has consequently been developed and has evolved during the last four centuries. However, the timbral properties that are described with natural language have not received a proper formalization and are still unclear. For this reason, people who need to refer to the timbral properties, such as teachers, violin makers and students, often rely on metaphors and synaesthesias in their description, i.e., they use terms that were originally meant to describe a different sensory perception field, such as a \textit{sweet} or \textit{warm} \cite{Zanoni2014}.

Classical approaches to the study of sound properties of musical instruments consists of extracting and analyzing a large set of acoustic cues, i.e. the LLFs \cite{Kim2005}. Concerning the characterization of the sound quality of violins, in \cite{Kaminiarz2007,Charles2006} the authors use a set of MPEG spectral and harmonic descriptors, whereas in \cite{Lukasik2010}, the author uses long-term cepstral coefficients. Concerning other instruments, in \cite{Barthet2010} the authors investigate how a set of timbral, temporal, dynamics and pitch descriptors change for different expressive performance of clarinet player, while the authors of \cite{percival2013physical} use MFCCs and residual MFCCs to distinguish among different cello performers.

Such descriptors, however, are characterized by a low level of abstraction and are not feasible to be used by musicians and instrument makers, who are used to a semantically rich description for the sound quality of their instruments \cite{Disley2006}. The American National Standards Institute (ANSI) defines the timbre as \textit{that attribute of auditory sensation in terms of which a listener can judge that two sounds, similarly presented and having the same loudness and pitch, are different} \cite{american1960usa}. This is an anti-definition, since it defines the timbre as the property to distinguish two sound and that is not loudness and pitch. Therefore, the timbre can be seen as a two-fold property of an instrument. On the one hand, it is a low-level property of the sound signal, which is objectively related to the property of the acoustic wave that is generated by the instrument, and it codes the information that allows the listeners to distinguish between two sounds. On the other hand, the sensory and intellectual elaboration of the timbre information of the listener, i.e., the perception of the timbre, leads to its high-level description. The semantic description of the timbre usually refers to a summary of several low-level properties that contribute to their high-level perception. As an example, a timbre might be defined as \textit{warm} when its spectrum is mainly distributed over the lower frequencies and the content is rather harmonic. The example is however just theoretical, since it is still obscure which low-level characteristics are involved in the perception process and how they are combined in the  ensemble timbral perception. 

The task of the automatic semantic annotation of the sound quality of violins is extremely complicated, due to the poor formalization and little knowledge we can infer from the involved domains. 
The signal domain originates from the sound of the instruments, but it is hard to understand the relevant properties that need to be captured. Due to the variety of timbral aspects we need to take into consideration, the definition and the design of a set of low-level features is problematic. 
The semantic domain is also difficult to formalize. Many terms are used to describe the timbre of violins and within the use case of music practicing and execution, it is common for a teacher to ask a student for a \textit{harder} or \textit{warmer} sound, which implicitly assumes the existence of a graded scale for the description, that needs to be modeled. 
Finally, the relationship between the signal domain and the semantic domain made with graded descriptors is unknown, since it is strongly affected by the human perception of the sound and by the training of the listener.

In order to address the complexity of the formalization of the signal domain, we use deep learning techniques to automatically extract a feature representation from the audio signal with different levels of abstraction. The approach was effectively employed in studies in Chapter \ref{Chap:MSA} and \ref{Chap:Bootleg} to learn respectively a local representation of the inner structure of the song and a global representation of the song that encodes some information on its generation process. In this work, we aim at learning a feature representation from violin recordings using an unsupervised approach.

With respect to the semantic domain, we propose a set of semantic musical descriptors that are used for describing the timbre of violins. The proposed semantic model follows a dimensional approach, which allows us to express the degree of intensity of each descriptor. A set of recordings of a number of violins (among them, Stradivari, Amati and Guarnieri instruments) are annotated with the descriptors through questionnaires by violin makers, who are among the most trained people to perceive even small differences in the timbre of violins.

Since the semantic domain is formalized as a dimensional semantic model, and due to the high complexity and uncertainty in the formalization of the signal domain, we model the linking function by means of regression techniques, as presented in Section \ref{sec:ML:semantics}. We do not have an insight on the kind of relation between the learned low-level representation and high-level description, hence we model the linking function by exploring different regression techniques, and we evaluate their performance by means of  objective quality metrics. This study was presented at the European Signal Processing Conference (EUSIPCO) \cite{Buccoli2015violin}. 


 \begin{figure}[tbp]
    \begin{center}
      \includegraphics[trim=0.8cm 5.6cm 0.8cm 5.2cm,clip=true,width=\textwidth]{img/Violin/schema2}
	%\psfig{file=im/ML/logopm.jpg,width=3.5cm}
    \end{center}
  \caption{The scheme of the application scenario for the automatic description with a dimensional semantic model}
  \label{fig:Violin:scheme}
  \end{figure}

In Figure \ref{fig:Violin:scheme} we summarize the current application scenario addressed in this Chapter. In the following we present the formalization of signal domain and semantic domain and the design of regression functions. We discuss the experimental setup for our approach and the numerical results we achieve.


\section{Formalization of the signal domain}\label{sec:Violin:features}
For our work, we composed a dataset $\mathcal{S}$ by recording a set of $N=28$ violins. We recorded thirteen historical violins (three Amati, two Guarnieri {\em del Ges\`u} and eight Stradivari) and fifteen modern violins from the collection of the ``Museo del Violino" in Cremona and ``International School of Lutherie" (Stradivari Institute) in Cremona, played by a professional musician according to a specific protocol. We obtained one recording for each violin.

It is worth discussing some details on the protocol followed for the recording of the violins, since the recording process might introduce artifacts that affect the sound quality of the recordings and hence its perceived timbre. We used high-quality recording systems and devices, which aim at minimizing their impact on the recorded audio, and the violins were recorded in a semi-anechoic room, in order to avoid reflection and reverberation of the sound waves that typically affect the spectral \textit{color} of the signal. Finally, since each musician has their own style of execution, which affects the final timbre \cite{percival2013physical}, all the recordings were conducted with a unique professional musician. 

In order to keep the same amount of data for each violin, we trimmed each recording to 30 seconds. We automatically extracted the low-level features by means of an unsupervised Deep Belief Network. We computed the visible input of the networks as in the previous Chapters by dividing the recordings into $50\%$-overlapping 1024-sample frames and computing the log-magnitude of the frequency spectrum for each frame. We implemented a three-layers DBN with 50 neurons per layer, using a training learning rate of $10^{-6}$ and $100$ epochs, since we have less data with respect to the 10,000 songs of Chapter \ref{Chap:MSA}. 

In \cite{hamel2011} it is shown that the features learned by a DBN can be made more robust to rapid variations and more effective for prediction by averaging the feature frames over intervals between 3 and 6 seconds. In this work, we average the frames over sliding overlapping $5$s-long segments. We obtain the final representation $\mathbf{h}^{(k)}_{i,a}$ where $a=1,..., A$ is the index of the sliding segment, $k$ is the index of the layer with $k=1,2,3$, for each song $s_i \in \mathcal{S}$. It is worth noticing that the number of segments $A$ is constant over all the recordings since they have been trimmed to the same duration.


%In Chapter \ref{Chap:Bootleg} we have averaged the frames over 30-second segments  to obtain a global representation for the whole excerpts, while in Chapter \ref{Chap:MSA} we use the intervals given by the beats instants, which are between 250 ms and 2 s (30 to 240 BPM).  


\begin{table}[tb]
\caption{Set of bipolar descriptors for violin timbre description.}
\label{tab:Violin:descriptors}
\centering
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{||l||l|l|l|l|l|l||}
\hline
\hline
$t_p^{(l)}$ & Dark & Hard & Not Deep & Not Warm & Not Full & Harsh \\
 $t_p^{(h)}$ & Bright & Soft & Deep & Warm & Full & Sweet\\
\hline
\hline
\end{tabular}
\egroup
\end{table}

\section{Formalization of the semantic domain}\label{sec:Violin:HLFs}
In \cite{Zanoni2014} the authors investigate the most used terms by means of survey to a large set of violin makers. In our study we select a set of $P=6$ dimensional bipolar semantic descriptors from \cite{Zanoni2014} in order to characterize the sound quality of violins. Each descriptor $t_p$ represents an aspect of the sound quality and it is identified by a pair of opposite terms $t_p^{(l)}$ and $t_p^{(h)}$. The selected descriptors are listed in Table \ref{tab:Violin:descriptors}. Given a generic violin recording $s_i$, we formalize its semantic annotation as $\mathbf{y}_i\in [0,1]^P$, where each component $\mathbf{y}_i^{(p)}$ represents the degree of descriptiveness of the $p$-th term for the $i$-th recording.  A low value of  $\mathbf{y}_i^{(p)}$, down to $0$, means that the violin is properly described by $t_p^{(l)}$, whereas a higher value, up to $1$, represent that it is described by $t_p^{(h)}$. The middle value of $0.5$ represents that in the recording there is not a strict prevalence of neither bipolar descriptor.  By using the dimensional approach, we aim at providing a higher degree of freedom and accuracy in the description. 

We collected annotations for each recording (i.e., for each instrument) by means of a questionnaire that was proposed to four professional violin makers. The testers were asked to annotate the recordings using a 11-point scale ranging from 0 (total prevalence of $t_p^{(l)}$) to 10 (total prevalence of $t_p^{(h)}$) for each defined  descriptor. The annotations were averaged over the subjects and normalized between 0 and 1 to match the range defined for the semantic model. We composed the final annotation dataset as
\begin{equation}
\mathcal{Y}=\{\mathbf{y}_1,..., \mathbf{y}_N  \}.
\end{equation}


\section{Design of the linking function} \label{subsec:Violin:regression}
We could model the linking function as a classifier by reducing the range of $\mathbf{y}_p$ into a set of discrete bins. Nevertheless, this would also limit the descriptive capability of our approach. For this reason, we decide to apply regression analysis, which is used to predict a real value from a set of observed variable by projecting a multidimensional feature space into a novel continuous space with a limited number of dimensions \cite{Yang2008}. In our case, for each semantic descriptor, the learned feature space is mapped into a novel conceptual one-dimensional space of real values (HLF). 

%Since we have only $28$ recording, 
We expand our dataset by considering the sliding segments defined in Section \ref{sec:Violin:features}, and we define the dataset as:
\begin{equation}
\mathcal{D}=\{(\mathbf{h}_{1,1},  \mathbf{y}_1),..., (\mathbf{h}_{1,A},  \mathbf{y}_1),..., \mathbf{h}_{N,1},  (\mathbf{y}_N),..., (\mathbf{h}_{N,A},  \mathbf{y}_N) \}.
\end{equation}
The final corpus is composed of 700 segments. 

Since it is not clear the correlation between the signal domain and the semantic domain and we do not know in advance which regression technique is more feasible to model the semantic descriptors, nor whether the same technique performs well for all the descriptors, in this study we experiment several regression functions, which have been discussed in Section \ref{sec:ML:semantics}: 
\begin{itemize}
\item Linear Regression (LR) \cite{Sen1990};
\item Ridge Regression (RR) \cite{Sen1990};
\item Polynomial Regression (PR) \cite{Sen1990};
\item Support Vector Regression (SVR) \cite{Rho2009} with RBF kernel (Section \ref{sec:ML:kernels}); 
\item Gradient Boosting Regression (GBR) \cite{Zemel2001} with depth 3;
\item Ada-Boosting Regression (ABR) \cite{Solomatine2004} with depth 3.
\end{itemize}

In the following Section we discuss the details of the experimental setup and we comment the achieved results.

\section{Experimental setup and numerical results}\label{sec:Violin:results}

\subsection{Experimental setup}\label{sec:Violin:setup}

In Chapters \ref{Chap:MSA} and \ref{Chap:Bootleg} we divided the dataset into three subsets for the training of the deep learning network, the computation of parameters of the linking function techniques and the final evaluation of performance. 

Given the small amount of data of this work, however, we divide the songs into only two sets, a training set $\mathcal{S}_{train}$ and a test set $\mathcal{S}_{test}$. The training set is composed of $20$ recordings and it is used both to learn the features with the deep learning technique and to learn the parameters of the regressors. We use the remaining $8$ recordings to compose the test set $\mathcal{S}_{test}$, which is used to evaluate the performance of the proposed approach.  

We need to train the DBN over a set of samples in order to learn the distribution of an input variable, i.e., the probability that a certain sample belongs to such distribution. Since we want to focus on the timbre of violins, we need to use only recordings of violin to train the DBN, and we cannot rely on the big dataset of songs used in Chapter \ref{Chap:MSA}. 

Due to the shortage of available data, we used a k-fold cross validation  to evaluate our approach, i.e., we iterate our experiment 5 times, each time randomly selecting a different $\mathcal{S}_{train}$ and $\mathcal{S}_{test}$, and we average the performance metrics over the k-fold iterations. In the following discussion, we explain the procedure we use for each iteration.

We randomly select $20$ recordings to compose the $\mathcal{S}_{train}$ and the remaining $8$ to compose the $\mathcal{S}_{test}$. From the randomly selected sets, we compose the annotated datasets as:
\begin{equation}
\mathcal{D}_{train}=\{ (\mathbf{h}_{i,a}, \mathbf{y}_i) \; \forall a=1,...,A; \; \forall s_i \in \mathcal{S}_{train};  \};
\end{equation}
\begin{equation}
\mathcal{D}_{test}=\{ (\mathbf{h}_{i,a}, \mathbf{y}_i)\; \forall a=1,...,A; \; \forall s_i \in \mathcal{S}_{test} \}.
\end{equation}
We used the $\mathcal{D}_{train}$ to train each of the regressors defined in Section \ref{subsec:Violin:regression} for each of the bipolar descriptors described in Section \ref{sec:Violin:HLFs}. We then use the obtained parameters to compute the estimations over the test set 
\begin{equation}
\hat{\mathcal{D}}_{test}=\{ (\mathbf{h}_{j,a}, \hat{\mathbf{y}}_{j,a}) \; \forall a=1,...,A; \; \forall s_j \in \mathcal{S}_{test} \}.
\end{equation}
We obtain the final estimated test set $\hat{\mathcal{Y}}_{test}$ by averaging the predictions $\hat{\mathbf{y}}_{j,a}$ over the segments $a=1,...,A$, and we compute the performance metrics as the difference between the annotations in $\mathcal{Y}_{test}$. 

We evaluate the quality of prediction by means of two metrics, the Root Mean Square Error (RMSE) and the coefficient of determination $R^2$. The former is defined as:
\begin{equation}
RMSE_{p}=\sqrt{\frac{1}{|\mathcal{Y}_{test}|} \sum_{s_i \in \mathcal{S}_{test}}  (\mathbf{y}_i^{(p)}-\hat{\mathbf{y}}_i^{(p)} )^2  }.
\end{equation}
We use the root square in order to obtain an error range comparable with the range of the annotations.
The $R^2$ is computed as a measure of the proportion between the variances of the predicted values and of the annotated values, both with respect to the same average (i.e., the average of the annotations):
\begin{equation}
R^2_p=  1-\frac{\sum_{s_i \in \mathcal{S}_{test}}  (\hat{\mathbf{y}}_i^{(p)}-\bar{\mathbf{y}}^{(p)} )^2 }{\sum_{s_i \in \mathcal{S}_{test}}  (\mathbf{y}_i^{(p)}-\bar{\mathbf{y}}^{(p)} )^2 } \in ]-\infty, 1 ]; 
\end{equation}
\begin{equation}
\text{where }\; \bar{\mathbf{y}}^{(p)}=\frac{1}{|\mathcal{S}_{test}|}\sum_{s_i \in \mathcal{S}_{test}} \mathbf{y}_i^{(p)}
\end{equation}
 is the average of the annotations \cite{Sen1990}. The $R^2$ index is a standard metric that measures the accuracy of the regression model. Positive values, up to $1$ indicates a high quality regression, while negative values indicate completely wrong regression. The value $R^2=0$ indicates a regressor that is randomly predicting the average of the annotations.

We train the regression model by using the learned features from each layer of the DBN $\mathbf{h}^{(k)} \in \mathbb{R}^{50} $ with $k=1,2,3$ as well as all the complete representation  $\mathbf{h}^{(all)}=[\mathbf{h}^{(1)};\mathbf{h}^{(2)};\mathbf{h}^{(3)}] \in \mathbb{R}^{150}$

\begin{table}[!tbp] %bh]
\caption{Numerical results for the RMSE metrics. For each descriptor, best results are in bold green, while worst results are in red italics.}
\label{tab:Violin:allRMSE}
\centering
%\large
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{||l|c|c|c|c|c|c|c||}
\hline
\hline
Descriptor & k & LR & RR & PR & SVR & GBR & ABR\\
\hline
\hline
 & 1 & 0.1448 & \color[HTML]{326B00} \textbf{0.1274} & 0.1431 & 0.1381 & 0.1354 & 0.1345 \\
Dark & 2 & 0.1469 & 0.1295 & 0.1356 & 0.1392 & 0.1349 & 0.1396 \\
Bright & 3 & 0.1413 & 0.1348 & 0.1376 & 0.1388 & 0.1518 & 0.1481 \\
 & ALL & \color[HTML]{8E0000} \textit{0.1598} & 0.1277 & 0.1542 & 0.1337 & 0.1375 & 0.1428 \\
\hline
\hline
 & 1 & 0.1623 & 0.1378 & 0.1607 & 0.1447 & 0.1478 & 0.1351 \\
Not Warm & 2 & 0.1666 & 0.1416 & 0.1538 & 0.1489 & 0.1488 & 0.1323 \\
Warm & 3 & 0.1632 & 0.1473 & 0.1449 & 0.1453 & 0.1444 & \color[HTML]{326B00} \textbf{0.1319} \\
 & ALL & \color[HTML]{8E0000} \textit{0.2045} & 0.1403 & 0.1748 & 0.1452 & 0.1497 & 0.1358 \\
\hline
\hline
 & 1 & 0.1865 & 0.1377 & 0.1771 & 0.1434 & 0.1444 & 0.1303 \\
Harsh & 2 & 0.1907 & 0.1348 & 0.1623 & 0.1304 & 0.1444 & 0.1277 \\
Sweet & 3 & 0.1853 & 0.1365 & 0.1500 & 0.1430 & 0.1464 & 0.1286 \\
 & ALL & \color[HTML]{8E0000} \textit{0.2220} & 0.1378 & 0.1935 & 0.1413 & 0.1410 & \color[HTML]{326B00} \textbf{0.1269} \\
\hline
\hline
 & 1 & 0.1994 & 0.1561 & 0.1886 & 0.1670 & 0.1563 & 0.1494 \\
Not Full & 2 & 0.2059 & 0.1556 & 0.1800 & 0.1736 & 0.1491 & \color[HTML]{326B00} \textbf{0.1326} \\
Full & 3 & 0.2039 & 0.1560 & 0.1721 & 0.1676 & 0.1529 & 0.1365 \\
 & ALL & \color[HTML]{8E0000} \textit{0.2463} & 0.1593 & 0.2138 & 0.1691 & 0.1468 & 0.1328 \\
\hline
\hline
 & 1 & 0.1246 & 0.0992 & 0.1203 & 0.1056 & 0.1014 & 0.1030 \\
Hard & 2 & 0.1275 & 0.0999 & 0.1148 & 0.1026 & 0.0997 & 0.0940 \\
Soft & 3 & 0.1267 & 0.1002 & 0.1090 & 0.1019 & 0.0948 & \color[HTML]{326B00} \textbf{0.0860} \\
 & ALL & \color[HTML]{8E0000} \textit{0.1721} & 0.0974 & 0.1360 & 0.1039 & 0.0962 & 0.0900 \\
\hline
\hline
 & 1 & 0.1971 & 0.1466 & 0.1866 & 0.1552 & 0.1524 & 0.1404 \\
Not Deep & 2 & 0.2017 & 0.1480 & 0.1789 & 0.1593 & 0.1537 & 0.1442 \\
Deep & 3 & 0.2030 & 0.1457 & 0.1741 & 0.1554 & 0.1564 & \color[HTML]{326B00}\textbf{0.1401} \\
 & ALL & \color[HTML]{8E0000} \textit{0.2363} & 0.1532 & 0.2098 & 0.1563 & 0.1534 & 0.1426 \\
\hline
\hline
\end{tabular}
\egroup
\end{table}

\begin{table}[tbp] %bh]
\caption{Numerical results for the $R^2$ metrics. For each descriptor, best results are in bold green, while worst results are in red italics.}
\label{tab:Violin:allR2}
\centering
%\large
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{||l|c|c|c|c|c|c|c||}
\hline
\hline
Descriptor & k & LR & RR & PR & SVR & GBR & ABR\\
\hline
\hline
 & 1 & -0.0914 & \color[HTML]{326B00}\textbf{0.1476} & -0.0685 & 0.0070 & 0.0303 & 0.0337 \\
Dark & 2 & -0.1226 & 0.1168 & 0.0312 & -0.0317 & 0.0326 & -0.0357 \\
Bright & 3 & -0.0340 & 0.0554 & 0.0058 & -0.0117 & -0.2029 & -0.1472 \\
 & ALL & \color[HTML]{8E0000}\textit{-0.3278} & 0.1461 & -0.2294 & 0.0547 & 0.0041 & -0.0882 \\
\hline
\hline
 & 1 & 0.0194 & 0.2969 & 0.0279 & 0.2281 & 0.1908 & 0.3275 \\
Not Warm & 2 & -0.0206 & 0.2557 & 0.1135 & 0.1772 & 0.1767 & 0.3477 \\
Warm & 3 & 0.0243 & 0.1888 & 0.2117 & 0.2055 & 0.2169 & \color[HTML]{326B00}\textbf{0.3485} \\
 & ALL & \color[HTML]{8E0000}\textit{-0.5781} & 0.2689 & -0.1549 & 0.2156 & 0.1545 & 0.3089 \\
\hline
\hline
 & 1 & -0.2467 & 0.3242 & -0.1229 & 0.2808 & 0.2527 & 0.4008 \\
Harsh & 2 & -0.2932 & 0.3519 & 0.0554 & 0.3982 & 0.2421 & 0.4137 \\
Sweet & 3 & -0.2205 & 0.3341 & 0.1906 & 0.2816 & 0.2208 & 0.4014 \\
 & ALL & \color[HTML]{8E0000}\textit{-0.7861} & 0.3178 & -0.3288 & 0.3016 & 0.2741 & \color[HTML]{326B00}\textbf{0.4182 }\\
\hline
\hline
 & 1 & -0.1582 & 0.3033 & -0.0325 & 0.2120 & 0.2908 & 0.3576 \\
Not Full & 2 & -0.2387 & 0.3031 & 0.0587 & 0.1392 & 0.3498 & \color[HTML]{326B00}\textbf{0.4912} \\
Full & 3 & -0.2100 & 0.3039 & 0.1439 & 0.1970 & 0.3290 & 0.4652 \\
 & ALL & \color[HTML]{8E0000}\textit{-0.7634} & 0.2711 & -0.3285 & 0.1917 & 0.3759 & 0.4892 \\
\hline
\hline
 & 1 & -0.0266 & 0.4160 & 0.0532 & 0.3432 & 0.3678 & 0.3436 \\
Hard & 2 & -0.0736 & 0.3855 & 0.1304 & 0.3380 & 0.3728 & 0.4663 \\
Soft & 3 & -0.0509 & 0.3720 & 0.2271 & 0.3560 & 0.4501 & \color[HTML]{326B00}\textbf{0.5561} \\
 & ALL & \color[HTML]{8E0000}\textit{-1.0029} & 0.4273 & -0.2274 & 0.3503 & 0.4172 & 0.5027 \\
\hline
\hline
 & 1 & -0.4377 & 0.2305 & -0.2888 & 0.1333 & 0.1492 & \color[HTML]{326B00}\textbf{0.2754} \\
Not Deep & 2 & -0.5031 & 0.2097 & -0.1961 & 0.0831 & 0.1059 & 0.2071 \\
Deep & 3 & -0.5069 & 0.2312 & -0.1188 & 0.1311 & 0.0577 & 0.2400 \\
 & ALL & \color[HTML]{8E0000}\textit{-1.0826} & 0.1483 & -0.6724 & 0.1212 & 0.0876 & 0.2111 \\
\hline
\hline
\end{tabular}
\egroup
\end{table}


\subsection{Numerical results} 
We present the results for RMSE and $R^2$ metrics in Tables \ref{tab:Violin:allRMSE} and \ref{tab:Violin:allR2} respectively. 

With regard to the RMSE, in Table \ref{tab:Violin:allRMSE}, we see that the RMSE ranges between the best results of $0.0860$, when predicting the \textit{Soft - Hard} descriptor, to $0.2463$ when trying to predict how much a violin is \textit{Full}. The results are in general promising, since even the maximum error is not excessive. We notice from the green and bold highlighting of the best results for each descriptor that the Ada-Boost Regression is the most effective regression algorithm, while the Linear Regression always achieves the worst result. With respect to the layer of the network involved in the prediction, the results of Linear Regression are especially low when using all the layers together. Nevertheless, this representation is also able to achieve the highest result when estimating the \textit{Harsh - Sweet} descriptors.

We believe that the Linear Regressor is too rigid to learn a proper model for estimating the sound quality of the violin, and the set of all layers might lead to an overfitting issue, due to the small dataset. The LR performance is, indeed, far worse when using all the layers together than when using each single layers,
On the other hand, the ABR achieves the best results with the third layer on $50 \%$ of the times. This confirms the assumption of the timbral quality as a property whose semantic description requires a complex combination of low-level features.
The use of nonlinear kernels in the PR (polynomial kernel) and SVR (RBF kernel) does not seem to improve the quality of the prediction, since the two algorithms never achieve the best performance, which are, in fact, quite average. The Ridge Regression, instead, is able to correct the Linear Regression and address the overfitting issue, by achieving the best results for the \textit{Dark - Bright} descriptor. Moreover, the best results are achieved with the first learned layer, hence the least abstract one. We can infer that the sound quality of darkness is more related to the lower-level properties of the audio signal than the other ones, which is confirmed also by the good results obtained by the first layer with ABR (best) and GBR (second best). 

While the RMSE and $R^2$ metrics are highly correlated, they are not strictly proportional, hence some results are slightly different, e.g., the best $R^2$  result for the \textit{Deep} quality is achieved by ABR with the first layer, and with the third layer for RMSE. From Table \ref{tab:Violin:allR2} we estimate in fact  the overall quality of the regression technique. As an example, we have discussed the poor results of the LR: as a matter of fact, it mostly obtains negative values of $R^2$. The ABR proves its effectiveness also when measuring its $R^2$ performance. We also notice that Ridge Regressor is often the second or third best-performing algorithm. Since the RR is a simple Linear Regression with $L^2$ regularization, we can infer that some learned features might be misleading for the prediction. LR, PR and SVR, in fact, do not employ any kind of regularization or feature selection, and they often obtain poor or below-average results, while RR employs a regularization and GBR and ABR assign a weight of relevance to certain samples and are able to learn an ensemble of methods that prove to be more robust to our case. We believe that the need of feature selection raises from the use of the unsupervised approach to learn features. This approach aims at representing the input as most general as possible, while our task is highly specific and we use a limited dataset.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.1\columnwidth]{img/Violin/r2_new}%bestr2_}
	\caption{Best performance for each descriptor for each prediction model in terms of $R^2$}
	\label{fig:Violin:R2}
\end{figure}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.1\columnwidth]{img/Violin/RMSE_new}%bestr2_}
	\caption{Best performance for each descriptor for each prediction model in terms of RMSE}
	\label{fig:Violin:RMSE}
\end{figure}

Finally, Figures \ref{fig:Violin:R2} and \ref{fig:Violin:RMSE} show the best performance for each prediction model and descriptor in terms of $R^2$ and RMSE. As it can be noticed, Ada-Boost, Gradient Boost and Ridge Regression outperform the other methods. From this general view, we can also infer which descriptors are easier to model. It is clear that the semantics related to the \textit{Hard - Soft} and \textit{Full} is rather clear and easier to be modeled, while the proposed predictors for the \textit{Dark - Bright} or the \textit{Deep} qualities are not very reliable. We can therefore assess that the same set of predictors with the same learned-feature representation achieve rather different performance across the descriptors. We believe that this is mainly due to the consensus on the semantics of the terms among the violin makers. In their community, in fact, the meaning of \textit{Deep} is not as much clear as \textit{Hard - Soft} is. This makes the annotations less reliable and a larger dataset of recordings and annotation would be required for further experiments. However, the overall accuracy of the method is promising and some descriptors are predicted with a high degree of precision.


%
%From Table \ref{tab:Violin:results} we can also noticed that different descriptors are better modeled by a different level of the DBN: \textit{Bright / Dark} is better modeled with the features learned in the first layer, whereas \textit{Sweet / Harsh} descriptor is better modeled with the collection of features learned at all the levels. Intuitively, there is a connection between these results and the level of abstraction of the descriptors. \textit{Bright / Dark}, in fact, can be easily related to the behavior of some spectral LLFs. Instead the meaning of \textit{Sweet / Harsh} tends to be more abstract.
%

%The obtained performance is far from perfection. In fact, deep learning techniques require a tremendous amount of data in order to obtain good results. %for the unsupervised training of features. 
%Nevertheless, the overall accuracy in automatic annotation is promising. It is also worth remembering that learned features have been obtained by means of a totally unsupervised approach and better results may be achieved with a further fine-tuning step.

%The overall accuracy in automatic annotation is promising. Thank to the layered representation of the DBN, we can also infer some information on the relationship between the semantic descriptors and the underlying low-level signal. 


%It is also worth remembering that learned features have been obtained by means of a totally unsupervised approach and better results may be achieved with a further fine-tuning step
%
%The obtained performance is far from perfection. In fact, deep learning techniques require a tremendous amount of data in order to obtain good results. %for the unsupervised training of features. 
%Nevertheless, 
%

\section{Final considerations}\label{sec:Violin:conclusion}
In this Chapter we presented an application scenario for modeling a set of 6 bipolar semantic descriptors for the sound quality of violins. Through a set of regression functions, the method builds a model that predicts real values from a large set of features, learned by means of a Deep Belief Network method.

While the models that we obtained turn out to be quite accurate over a large set of descriptors, it is worth discussing the limits of the approach. Machine learning techniques --- especially Deep Learning ones --- require a tremendous amount of data to be properly trained, while in this scenario we only had $28$ recordings of about $30$ seconds each. Moreover, the semantics of the descriptors was sometimes fuzzy, so it was hard to reach a consensus among annotators. 

Nevertheless, we were able to address the highly complex model of the annotation of violins recordings. The formalization of the semantic domain allows us to create a dimensional semantic model in which we define a continuous description of the timbre of violins. As an example, even if we discretized each bipolar descriptor on a 11-level scale, we would still be able to represent $11^6$ different overall timbres. The signal domain was also highly complex to identify, and the deep learning techniques helped tackling the issue. Finally, the regression techniques provided a reliable and automatic solution for the modeling of the linking function. We believe that we could increase the performance of our approach by enriching the dataset of violin recordings.



%\section{Acknowledgments}
%We are grateful to the Violin Museum Foundation and the Stradivari International School of Lutherie (both in Cremona, Italy), for supporting the activities of timbral acquisitions on historic violins. We are particularly indebted with Fausto Cacciatori and Alessandro Voltini, for their patient work with us during the timbral acquisitions. We would also like to thank the violin makers who helped us produce the annotations and the virtuoso violinist Anastasiya Petryshak who helped us produce the audio data for the analysis.
%
% 